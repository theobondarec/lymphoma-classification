{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ideal-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
    "\n",
    "import tkinter.filedialog as filedialog\n",
    "import tkinter as tk\n",
    "import time\n",
    "\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "\n",
    "image_width, image_height = 224, 224\n",
    "weights_path = r'weights-improvement-05-0.138-0.970.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "optional-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(input_file, height, width):\n",
    "    img = PIL.Image.open(input_file)\n",
    "    img_width, img_height = img.size\n",
    "    crop_img = []\n",
    "    if img_height%height == 0 :\n",
    "        i_iter = img_height//height\n",
    "    else :\n",
    "        i_iter = img_height//height+1\n",
    "    \n",
    "    if img_width%width == 0 :\n",
    "        j_iter = img_width//width\n",
    "    else :\n",
    "        j_iter = img_width//width+1\n",
    "        \n",
    "    for i in range(i_iter):\n",
    "        for j in range(j_iter):\n",
    "            box = (j*width, i*height, (j+1)*width, (i+1)*height)\n",
    "            crop_img.append(img.crop(box))\n",
    "    \n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "latter-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inceptionV3 = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(inceptionV3)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "    model.add(Dense(512,activation=('relu')))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(256,activation=('relu')))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(128,activation=('relu')))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(64,activation=('relu')))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(3,activation=('softmax')))\n",
    "    \n",
    "    for layer in model.layers[:-50]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(optimizer=Adam(lr=.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "tested-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(path):    \n",
    "    predict = []\n",
    "\n",
    "    for index, img in enumerate(crop(path, 224, 224)):\n",
    "        img_array = img_to_array(img)/255\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        predict.append(np.argmax(model.predict(img_array), axis=-1)[0])\n",
    "        print(model.predict(img_array))\n",
    "        \n",
    "    predict_dict = dict(Counter(predict))\n",
    "    \n",
    "#     print(predict_dict)\n",
    "    print('Classification : ')\n",
    "    try:\n",
    "        print('CLL :', predict_dict[0]/len(predict)*100, '%')\n",
    "        X = predict_dict[0]/len(predict)*100\n",
    "    except:\n",
    "        print('CLL :', '0 %')\n",
    "        X = 0\n",
    "    try:\n",
    "        print('FL :', predict_dict[1]/len(predict)*100, '%')\n",
    "        Y = predict_dict[1]/len(predict)*100\n",
    "    except:\n",
    "        print('FL :', '0 %')\n",
    "        Y = 0\n",
    "    try:    \n",
    "        print('MCL :', predict_dict[2]/len(predict)*100, '%')\n",
    "        Z = predict_dict[2]/len(predict)*100\n",
    "    except:\n",
    "        print('MCL :', '0 %')\n",
    "        Z = 0\n",
    "        \n",
    "    nl = '\\n'\n",
    "    \n",
    "    print(path)\n",
    "    print(predict)\n",
    "    prediction_result[\"text\"]=f\"Result : {nl} CLL : {round(X,2)}% {nl}FL : {round(Y,2)}% {nl} MCL : {round(Z,2)}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02611853 0.09428026 0.8796012 ]]\n",
      "[[6.2416314e-07 1.4745599e-06 9.9999785e-01]]\n",
      "[[4.5420738e-06 2.1147327e-05 9.9997425e-01]]\n",
      "[[3.6346689e-06 1.2237072e-05 9.9998415e-01]]\n",
      "[[6.6424855e-06 2.9277262e-05 9.9996412e-01]]\n",
      "[[1.7574255e-04 5.8528531e-04 9.9923897e-01]]\n",
      "[[8.7589593e-05 1.3701689e-04 9.9977535e-01]]\n",
      "[[9.5546078e-05 1.7778206e-04 9.9972671e-01]]\n",
      "[[7.423574e-05 4.523873e-04 9.994734e-01]]\n",
      "[[1.8026218e-09 8.9662997e-09 1.0000000e+00]]\n",
      "[[4.5659076e-04 9.3925511e-04 9.9860412e-01]]\n",
      "[[2.6172968e-06 1.2450218e-05 9.9998498e-01]]\n",
      "[[5.9640931e-04 1.2243927e-03 9.9817920e-01]]\n",
      "[[8.9079340e-06 1.3473406e-05 9.9997759e-01]]\n",
      "[[1.9486279e-04 9.0038183e-04 9.9890471e-01]]\n",
      "[[1.9060368e-06 5.5555206e-06 9.9999249e-01]]\n",
      "[[5.7616813e-07 2.4913393e-06 9.9999690e-01]]\n",
      "[[7.2545622e-06 2.3816097e-05 9.9996889e-01]]\n",
      "[[3.2662920e-10 1.4938817e-09 1.0000000e+00]]\n",
      "[[5.8851794e-05 3.2458626e-04 9.9961650e-01]]\n",
      "[[1.5641842e-07 4.8137923e-07 9.9999940e-01]]\n",
      "[[7.4608206e-06 2.0027082e-05 9.9997246e-01]]\n",
      "[[7.4245392e-05 5.0072593e-04 9.9942505e-01]]\n",
      "[[3.9356406e-04 2.6521415e-03 9.9695432e-01]]\n",
      "[[1.4854797e-05 2.9218892e-05 9.9995589e-01]]\n",
      "[[1.0830505e-13 1.5566335e-12 1.0000000e+00]]\n",
      "[[9.7455666e-04 3.4747946e-03 9.9555063e-01]]\n",
      "[[2.9530322e-06 9.1088441e-06 9.9998796e-01]]\n",
      "[[0.00510799 0.00988444 0.9850075 ]]\n",
      "[[0.00261615 0.02196762 0.97541624]]\n",
      "[[1.0910783e-10 5.9801586e-10 1.0000000e+00]]\n",
      "[[2.3750620e-17 6.2060865e-16 1.0000000e+00]]\n",
      "[[2.3340605e-17 2.5547379e-16 1.0000000e+00]]\n",
      "[[6.11747311e-08 1.18509135e-07 9.99999762e-01]]\n",
      "[[0.6125731  0.02293899 0.36448783]]\n",
      "Classification : \n",
      "CLL : 2.857142857142857 %\n",
      "FL : 0 %\n",
      "MCL : 97.14285714285714 %\n",
      "C:/Users/theob/OneDrive/Documents/1.Moi/ISEN/M1/S2/Project lymphoma classification/FL.tif\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "nl = '\\n'\n",
    "X = \"??\"\n",
    "Y = \"??\"\n",
    "Z = \"??\"\n",
    "\n",
    "window = tk.Tk()\n",
    "\n",
    "def bar():\n",
    "    progress['value'] = 20\n",
    "    window.update_idletasks()\n",
    "    time.sleep(1)\n",
    "  \n",
    "    progress['value'] = 40\n",
    "    window.update_idletasks()\n",
    "    time.sleep(1)\n",
    "  \n",
    "    progress['value'] = 50\n",
    "    window.update_idletasks()\n",
    "    time.sleep(1)\n",
    "  \n",
    "    progress['value'] = 60\n",
    "    window.update_idletasks()\n",
    "    time.sleep(1)\n",
    "  \n",
    "    progress['value'] = 80\n",
    "    window.update_idletasks()\n",
    "    time.sleep(1)\n",
    "    progress['value'] = 100\n",
    "    \n",
    "    \n",
    "def input():\n",
    "    filename = filedialog.askopenfilename(parent=window,title='Please select a file')\n",
    "    if filename:\n",
    "        bar()\n",
    "        prediction(filename)\n",
    "\n",
    "\n",
    "window.minsize(500, 300)\n",
    "window.columnconfigure(0, weight=1, minsize=100)\n",
    "window.columnconfigure(1, weight=3, minsize=250)\n",
    "window.columnconfigure(2, weight=1, minsize=100)\n",
    "\n",
    "\n",
    "window.rowconfigure([0, 1, 2], minsize=100)\n",
    "window.title(\"lymphoma classification prediction\")\n",
    "               \n",
    "\n",
    "input_path = tk.Label(text=\"Input File Path:\")\n",
    "input_path.grid(row=0, column=0, padx=5)\n",
    "input_entry = tk.Entry(text=\"\", width=40)\n",
    "input_entry.grid(row=0, column=1, padx=5)\n",
    "browse1 = tk.Button(text=\"Select and lanch\", command=input)\n",
    "browse1.grid(row=0, column=2, padx=5)\n",
    "\n",
    "\n",
    "progress = Progressbar(orient = HORIZONTAL, length = 400, mode = 'determinate')\n",
    "progress.grid(row=1, columnspan=3)\n",
    "\n",
    "\n",
    "prediction_result = tk.Label(text=f\"CLL : {X}% {nl}FL : {Y}% {nl} MCL : {Z}%\")\n",
    "prediction_result.grid(row=2, column=1, pady=5)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-reason",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
